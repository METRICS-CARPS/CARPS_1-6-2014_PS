---
title: "CARPS Reproducibility Report"
output:
html_document:
toc: true
toc_float: true
---
  
[PILOT/COPILOT - TEXT IN SQUARE BRACKETS IS HERE FOR GUIDANCE. COPILOT PLEASE DELETE BEFORE KNITTING THE FINAL REPORT]

# Report Details

[PILOT/COPILOT ENTER RELEVANT REPORT DETAILS HERE]

```{r}
articleID <- "CARPS_1-6-2014_PS" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- "final" # specify whether this is the 'pilot' report or 'final' report
pilotNames <- "Tysen Dauer, Erik Santoro, Jaclyn Schwartz" # insert the pilot's name here e.g., "Tom Hardwicke".  If there are multiple cpilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "Michele Nuijten" # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- NA # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- NA # insert the co-pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("10/26/17", format = "%m/%d/%y") # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("07/03/18", format = "%m/%d/%y") # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- NA # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

-------
  
#### Methods summary: 

[PILOT/COPILOT write a brief summary of the methods underlying the target outcomes written in your own words]

Mueller et al. 2014 aimed to find out if note-taking medium (longhand or laptop) had an effect on participants' ability to correctly answer factual and conceptual questions. Study 1 asked 67 participants to watch one of five different lectures, perform distractor tasks, and then answer questions about the lecture they viewed. They found that note-taking medium had a significant effect on Conceptual answers but not on Factual answers.

The authors report converting the raw data to z scores before listing the ANOVA results from 4 tests: 
1. Factual-recall questions by condition (laptop or longhand),
2. Conceptual-application questions by condition,
3. Conceptual-application affected by lecture (labelled "whichtalk" in the data), and 
4. Interaction between lecture and note-taking medium (labelled "condition" in the data).
According to the article, all of these ANOVAs were mixed fixed- and random-effects ANOVAs, with note-taking medium (laptop vs. longhand) as a fixed effect and lecture (which talk was viewed) as a random effect.

Before reproducing the ANOVAs, I reproduced the means and standard deviations reported for the raw and z scored data. 

------

#### Target outcomes: 

[PILOT copy and paste the target outcomes identified in targetOutcomes.md]  

For this article you should focus on the findings reported for Study 1 in section "Laptop versus longhand performance".

Specifically, you should attempt to reproduce all descriptive and inferential analyses reported in the text below and associated tables/figures:

> Laptop versus longhand performance. Mixed fixed and random-effects analyses of variance were used to test differences, with note-taking medium (laptop vs.longhand) as a fixed effect and lecture (which talk was viewed) as a random effect. We converted the raw data to z scores because the lecture assessments varied in difficulty
and number of points available; however, results did not differ when raw scores were analyzed.4 On factual-recall questions, participants performed equally well across conditions (laptop: M = 0.021, SD = 1.31; longhand: M = 0.009, SD = 1.02), F(1, 55) = 0.014, p = .91. However, on conceptual-application questions, laptop participants performed significantly worse (M = −0.156, SD = 0.915) than longhand participants (M = 0.154, SD = 1.08), F(1, 55) = 9.99, p = .03, ηp2 = .13 (see Fig. 1).5 Which lecture participants saw also affected performance on conceptual-application questions, F(4, 55) = 12.52, p = .02, ηp2 = .16; however, there was no significant interaction between lecture and note-taking medium, F(4, 55) = 0.164, p = .96. (from Mueller & Oppenheimer, 2014, p. 1161)

**Note**
Make sure to use the original article for additional context and information about any necessary pre-processing steps. Also check for additional supplementary materials that may provide supporting documentation for analysis procedures. 

------

[PILOT/COPILOT DO NOT CHANGE THE CODE IN THE CHUNK BELOW]  

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

[PILOT/COPILOT Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(CARPSreports) # custom report functions
```

[PILOT/COPILOT DO NOT MAKE CHANGES TO THE CODE CHUNK BELOW]

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

## Step 2: Load data

```{r}
data <- read_spss("data/Study_1_Upload_Data.sav")
```

## Step 3: Tidy data

```{r}
# make data tidier by gathering the variable of interest in long format
data_tidy <- data %>%
  gather("item_type_Z", "Z_score", objectiveZ, openZ, openandobjectiveZ)
```

## Step 4: Run analysis

### Pre-processing

[you can remove this section if no pre-processing is required]

```{r}
# A note about nomenclature: in the paper the authors refer to Factual and Conceptual questions. In the data these items are labelled "objective" and "open", respectively. 
```

### Descriptive statistics

From the article (emphasis added):

> On factual-recall questions, participants performed equally well
across conditions (laptop: **M = 0.021, SD = 1.31**; longhand: **M = 0.009, SD = 1.02**), [...] However, on conceptual-application questions, laptop participants performed significantly worse (**M = −0.156, SD = 0.915**) than longhand participants (**M = 0.154, SD = 1.08**). (from Mueller & Oppenheimer, 2014, p. 1161)

```{r}
descriptives <- data_tidy %>%
  group_by(condition, item_type_Z) %>%
  filter(item_type_Z != "openandobjectiveZ") %>%
  summarize(mean = mean(Z_score),
            sd = sd(Z_score))

descriptives

M1 <- descriptives %>%
  filter(condition == 0, item_type_Z == "objectiveZ") %>%
  ungroup() %>%
  select(mean)

SD1 <- descriptives %>%
  filter(condition == 0, item_type_Z == "objectiveZ") %>%
  ungroup() %>%
  select(sd)

M2 <- descriptives %>%
  filter(condition == 1, item_type_Z == "objectiveZ") %>%
  ungroup() %>%
  select(mean)

SD2 <- descriptives %>%
  filter(condition == 1, item_type_Z == "objectiveZ") %>%
  ungroup() %>%
  select(sd)

M3 <- descriptives %>%
  filter(condition == 0, item_type_Z == "openZ") %>%
  ungroup() %>%
  select(mean)

SD3 <- descriptives %>%
  filter(condition == 0, item_type_Z == "openZ") %>%
  ungroup() %>%
  select(sd)

M4 <- descriptives %>%
  filter(condition == 1, item_type_Z == "openZ") %>%
  ungroup() %>%
  select(mean)

SD4 <- descriptives %>%
  filter(condition == 1, item_type_Z == "openZ") %>%
  ungroup() %>%
  select(sd)
```

```{r}
reproCheck(reportedValue = "0.021",
           obtainedValue = M1,
           valueType = "mean")

reproCheck(reportedValue = "1.31", 
          obtainedValue = SD1,
          valueType = "sd")

reproCheck(reportedValue = "0.009",
           obtainedValue = M2,
           valueType = "mean")

reproCheck(reportedValue = "1.02", 
          obtainedValue = SD2,
          valueType = "sd")

reproCheck(reportedValue = "−0.156",
           obtainedValue = M3,
           valueType = "mean")

reproCheck(reportedValue = "0.915", 
          obtainedValue = SD3,
          valueType = "sd")

reproCheck(reportedValue = "0.154",
           obtainedValue = M4,
           valueType = "mean")

reproCheck(reportedValue = "1.08", 
          obtainedValue = SD4,
          valueType = "sd")

```


```{r}
# I also found their reported Raw Means for Study 1 and calculated them for comparison (these raw numbers are reported in the "Supplemental Material" folder, the file is named *Raw_Means_and_Questions.pdf).
raw.means <- data %>%
  group_by(condition, whichtalk)%>%
  summarize(mean.raw.objective = mean(rawobjective), 
            sd.raw.objective = sd(rawobjective), 
            mean.raw.open = mean(rawopen), 
            sd.raw.open = sd(rawopen))
```

Here too I found some difference that might help unravel later discrepancies. They seem to have different data for the condition:laptop lectures on Islam and Algorithms. Here is a verbal description. Please find the recorded errors repeated formally below.

In Factual questions, they reported the following [raw scores and (standard deviations)]: 
condition (laptop (=0)), whichtalk (Islam) [6.33 (1.75)] 
where I find [5.83 (2.48)]. 
Also in Factual questions, they reported: 
condition (laptop), whichtalk (Algorithms) [4.17 (2.14)]
where I find [4.67 (1.86)].

The laptop condition lectures on Islam and Algorithms also differed in Conceptual questions:
for Islam they report [3.50 (1.22)]
where I got [3.33 (1.03)]
and for Algorithm they report [3.50 (1.05)]
where I got [3.67 (1.21)].

Maybe they removed some subjects at this stage that make the standard deviation off and throw the later ANOVAs?

There is mention in the "Study 1 Participants" section of the paper of removing two participants. Those excluded participants seem not to be included in their public data (they report 67 participants minus the 2 = 65 participants, the number in their public data). I found no other mention of excluded data or differently aggregated data in the paper or supplemental materials.

I note the errors below but will not include them in the final count since they are not the Target Outcomes.

```{r}
# Recording error for Factual Islam laptop mean and sd.
##compareValues(reportedValue = 6.33, obtainedValue = 5.83) # mean
##compareValues(reportedValue = 1.75, obtainedValue = 2.48) # sd
```
```{r}
# Recording error for Factual Algorithm laptop mean and sd.
##compareValues(reportedValue = 4.17, obtainedValue = 4.67) # mean
##compareValues(reportedValue = 2.14, obtainedValue = 1.86) # sd

```
```{r}
# Recording error for Conceptual Islam laptop mean and sd.
##compareValues(reportedValue = 3.5, obtainedValue = 3.33) # mean
##compareValues(reportedValue = 1.22, obtainedValue = 1.03) # sd
```

```{r}
# Recording error for Conceptual Algorithm laptop mean and sd.
##compareValues(reportedValue = 3.5, obtainedValue = 3.67) # mean
##compareValues(reportedValue = 1.05, obtainedValue = 1.21) # sd
```


```{r echo=F, results= 'asis'}
knitr::kable(raw.means, caption = "Here is the data as a table. Note that laptop = 0 under condition, Islam = 1 under whichtalk, and Algorithms = 5 under whichtalk.")

```


### Inferential statistics

```{r}
# Libraries needed for ANOVA tests.
library(car)
library(lme4)
library(Matrix)
library(lmerTest)
library(lsr)
```

I label these sections using the numbering in Methods summary, above. The labels "a," "b," etc., are various attempts to reproduce the reported results.
1.a.
```{r}
# Deal with only Factual-recall questions (objectiveZ). 
# In the paper they got F(1,55) = 0.014, p =.91
# This gets close for the p-value: .952 (here, and below, the degrees of freedom are always higher than the paper's 55). The results also do not seem to include F-values.

model = lmer(objectiveZ ~ condition + (1|whichtalk),
             data = data)
summary(model)
Anova(model, type = "II")
```
```{r}
# Recording error for ANOVA p value.
#compareValues(reportedValue = .91, obtainedValue = .952, isP = T)
```


1.b.
```{r}
# Same as above (Factual = objectiveZ), with type="III". The results look identical to when type = "II".
model = lmer(objectiveZ ~ condition + (1|whichtalk),
             data = data)
summary(model)
Anova(model, type = "III")
```
1.c.
```{r}
# Alternative which did not work.
aovObj <- aov(objectiveZ ~ condition + Error(whichtalk/condition), data=data)
summary(aovObj)
```

1.d.
```{r}
# Another way of reporting the ANOVA, this one provides an F-value.
model = lmer(objectiveZ ~ condition + (1|whichtalk),
             data = data)
summary(model)
anova(model)
```
```{r}
# Recording error for ANOVA F value.
#compareValues(reportedValue = .014, obtainedValue = .003)
```


2.a.
```{r}
# Deals with Conceptual quesions (openZ). Here is the code with type = "II". 
# In the paper they get F(1, 55) = 9.99, p = .03
model = lmer(openZ ~ condition + (1|whichtalk),
             data = data)
summary(model)
Anova(model, type = "II")
```

2.b.
```{r}
# Same as above: testing Conceptual, this time with type = "III". This produced different, but still incorrect results. The results are also the same as when type = "II".
# In the paper they get F(1, 55) = 9.99, p = .03
model = lmer(openZ ~ condition + (1|whichtalk),
             data = data)
summary(model)
Anova(model, type = "III")
```

2.c.
```{r}
# Alternatives which did not work.
aovOp <- aov(openZ ~ condition + Error(whichtalk/condition), data=data)
summary(aovOp)
```

2.d.
```{r}
# An alternative attempt to deal with only Conceptual-application questions (openZ). Incorrect. 
aovConceptual <- aov(openZ ~ condition, data=data)
summary(aovConceptual)
```

2.e.
```{r}
# ANOVA run with F-value
model = lmer(openZ ~ condition + (1|whichtalk),
             data = data)
summary(model)
anova(model)
```
```{r}
# Recording error for ANOVA F value.
#compareValues(reportedValue = 9.99, obtainedValue = 1.74)
```

```{r}
# This was an attempt at investigating the eta squared value. I wa unable to get the function to work.
# etaSquared(model, type = 2, anova = TRUE)
```


3.
```{r}
# Lecture (whichtalk) affects performance on conceptual-application questions (openZ).
aovLecture <- aov(openZ ~ whichtalk, data=data)
summary(aovLecture)
```
4.
```{r}
# Interaction between lecture (whichtalk) and note-taking medium (condition). This could only be successful if the underlying Conceptual data is accurate, which it is not.
```

## Step 5: Conclusion

[Please include a text summary describing your findings. If this reproducibility check was a failure, you should note any suggestions as to what you think the likely cause(s) might be.]

[PILOT/COPILOT ENTER RELEVANT INFORMATION BELOW]

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

# How many of the above issues were resolved through author assistance?
locus_typo_resolved <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification_resolved <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis_resolved <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data_resolved <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified_resolved <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? TRUE, FALSE, or NA. This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

[PILOT/COPILOT DOD NOT EDIT THE CODE CHUNK BELOW]

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR")) | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified, locus_typo_resolved, locus_specification_resolved, locus_analysis_resolved, locus_data_resolved, locus_unidentified_resolved)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```


```{r}
#carpsReport(Report_Type = "pilot",
 #           Article_ID = "CARPS_1-6-2014_PS",
  #          Insufficient_Information_Errors = 0,
   #         Decision_Errors = 0,
    #        Major_Numerical_Errors = 3,
     #       Time_to_Complete = 525,
      #      Author_Assistance = FALSE)
```

The reported ANOVAs were the main findings. We encountered major errors already at the descriptive statistics stage. Hopefully these can be easily resolved by contacting the authors to find out what additional data processing, cleaning, or exclusions might be causing these differences. The innaccurate ANOVA findings recorded here (including the difference in reported degrees of freedom) may arise for the following reasons: the underlying data differences just described, incorrect R scripts for the ANOVAs reported in the article, or they may actually be different. The time to complete is possibly unusually long because I had not previously run mixed-effects ANOVAs and needed time and assistance implementing them in R. 

# Session information

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```

